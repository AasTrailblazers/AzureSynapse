{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Notebook Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(spark.conf.get(\"spark.executor.instances\"))\r\n",
        "print(spark.conf.get(\"spark.executor.cores\"))\r\n",
        "print(spark.conf.get(\"spark.executor.memory\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Note: If you request more number of vcores than  pool limit or available vcores in the pool, you will get an exception. Try reducing the numbers of vcores requested or increasing your pool size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f \r\n",
        "{\r\n",
        "    \"numExecutors\": 4, \r\n",
        "    \"executorCores\": 4,\r\n",
        "    \"executorMemory\": \"28g\"\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(spark.conf.get(\"spark.executor.instances\"))\r\n",
        "print(spark.conf.get(\"spark.executor.cores\"))\r\n",
        "print(spark.conf.get(\"spark.executor.memory\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-autoscale#get-started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\r\n",
        "    {\r\n",
        "        \"conf\" : {\r\n",
        "            \"spark.dynamicAllocation.enable\": \"true\",\r\n",
        "            \"spark.dynamicAllocation.minExecutors\": \"2\",\r\n",
        "            \"spark.dynamicAllocation.maxExecutors\" : \"6\"                        \r\n",
        "     }\r\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.opendatasets import NycTlcYellow\r\n",
        "\r\n",
        "from datetime import datetime\r\n",
        "from dateutil import parser\r\n",
        "start_date = parser.parse('2010-01-01')\r\n",
        "end_date = parser.parse('2010-02-28')\r\n",
        "\r\n",
        "nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
        "nyc_tlc_df = nyc_tlc.to_spark_dataframe()\r\n",
        "\r\n",
        "# nyc_tlc_df_clean = nyc_tlc_df.drop_duplicates()\r\n",
        "nyc_tlc_df_clean = nyc_tlc_df.drop_duplicates().repartition(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Converting Parquet to Delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "nyc_tlc_df_clean.write.mode(\"overwrite\").format(\"parquet\").save(\"/data/deltademo/parquettbl\")\r\n",
        "nyc_tlc_df_clean.write.mode(\"overwrite\").format(\"parquet\").partitionBy(\"puYear\",\"puMonth\").save(\"/data/deltademo/partitionedparquettbl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {}
      },
      "source": [
        "from delta.tables import *\r\n",
        "# Convert unpartitioned Parquet table at path '<path-to-table>'\r\n",
        "deltaTable = DeltaTable.convertToDelta(spark, \"parquet.`/data/deltademo/parquettbl`\")\r\n",
        "\r\n",
        "# Convert partitioned parquet table at path '<path-to-table>' and partitioned by columns with data type, separated by comma\r\n",
        "partitionedDeltaTable = DeltaTable.convertToDelta(spark, \"parquet.`/data/deltademo/partitionedparquettbl`\", \"puYear int, puMonth int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "-- Convert unpartitioned Parquet table at path '<path-to-table>'\r\n",
        "CONVERT TO DELTA parquet.`/data/deltademo/parquettbl`;\r\n",
        "\r\n",
        "-- Convert partitioned Parquet table at path '<path-to-table>' and partitioned by columns with data type, separated by comma\r\n",
        "CONVERT TO DELTA parquet.`/data/deltademo/partitionedparquettbl` PARTITIONED BY (puYear int, puMonth int);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Working with Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Creating managed delta tables\r\n",
        "# nyc_tlc_df_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"deltademo.deltataxitrips\")\r\n",
        "# nyc_tlc_df_clean.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"puYear\",\"puMonth\").saveAsTable(\"deltademo.deltapartitionedtaxitrips\")\r\n",
        "\r\n",
        "# Creating delta path/files\r\n",
        "nyc_tlc_df_clean.write.mode(\"overwrite\").format(\"delta\").save(\"/data/deltademo/deltataxitrips\")\r\n",
        "nyc_tlc_df_clean.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"puYear\",\"puMonth\").save(\"/data/deltademo/deltapartitionedtaxitrips\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "DROP DATABASE IF EXISTS deltademo CASCADE;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS deltademo\")\r\n",
        "spark.sql(\"CREATE TABLE IF NOT EXISTS deltademo.deltataxitrips USING DELTA LOCATION '{0}'\".format(\"/data/deltademo/deltataxitrips\")) \r\n",
        "spark.sql(\"CREATE TABLE IF NOT EXISTS deltademo.deltapartitionedtaxitrips USING DELTA LOCATION '{0}'\".format(\"/data/deltademo/deltapartitionedtaxitrips\")) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "CREATE DATABASE IF NOT EXISTS deltademo;\r\n",
        "\r\n",
        "CREATE TABLE IF NOT EXISTS deltademo.deltataxitrips\r\n",
        "USING DELTA\r\n",
        "LOCATION '/data/deltademo/deltataxitrips';\r\n",
        "\r\n",
        "CREATE TABLE IF NOT EXISTS deltademo.deltapartitionedtaxitrips\r\n",
        "USING DELTA\r\n",
        "LOCATION '/data/deltademo/deltapartitionedtaxitrips';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "DESCRIBE deltademo.deltataxitrips;\r\n",
        "DESCRIBE DETAIL deltademo.deltataxitrips;\r\n",
        "\r\n",
        "DESCRIBE deltademo.deltapartitionedtaxitrips;\r\n",
        "DESCRIBE DETAIL deltademo.deltapartitionedtaxitrips;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Data Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "SELECT puYear, puMonth, count(*)\r\n",
        "FROM deltademo.deltapartitionedtaxitrips \r\n",
        "GROUP BY puYear, puMonth;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from azureml.opendatasets import NycTlcYellow\r\n",
        "from datetime import datetime\r\n",
        "from dateutil import parser\r\n",
        "start_date = parser.parse('2010-02-01')\r\n",
        "end_date = parser.parse('2010-03-31')\r\n",
        "nyc_tlc_incr = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
        "nyc_tlc_incr_df = nyc_tlc_incr.to_spark_dataframe()\r\n",
        "\r\n",
        "nyc_tlc_incr_df_clean = nyc_tlc_incr_df.drop_duplicates().repartition(32)\r\n",
        "nyc_tlc_incr_df_clean.createOrReplaceTempView(\"deltaincrementaltaxitrips\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "MERGE INTO deltademo.deltapartitionedtaxitrips target\r\n",
        "USING deltaincrementaltaxitrips source\r\n",
        "ON \r\n",
        "  source.vendorID = target.vendorID AND source.tpepPickupDateTime = target.tpepPickupDateTime \r\n",
        "  AND source.tpepDropoffDateTime = target.tpepDropoffDateTime AND source.startLat = target.startLat \r\n",
        "  AND source.startLon = target.startLon AND source.endLat = target.endLat AND source.endLon = target.endLon \r\n",
        "  AND source.passengerCount = target.passengerCount AND source.tripDistance = target.tripDistance \r\n",
        "  AND source.rateCodeId = target.rateCodeId AND source.paymentType = target.paymentType \r\n",
        "  AND source.totalAmount = target.totalAmount\r\n",
        "  AND target.puYear IN (2010) AND target.puMonth IN (2,3) -- Partition Pruning \r\n",
        "WHEN MATCHED THEN\r\n",
        "  UPDATE SET \r\n",
        "    target.puLocationId = source.puLocationId\r\n",
        "    , target.doLocationId = source.doLocationId\r\n",
        "    , target.storeAndFwdFlag = source.storeAndFwdFlag\r\n",
        "    , target.fareAmount = source.fareAmount\r\n",
        "    , target.extra = source.extra\r\n",
        "    , target.mtaTax = source.mtaTax\r\n",
        "    , target.improvementSurcharge = source.improvementSurcharge\r\n",
        "    , target.tipAmount = source.tipAmount\r\n",
        "    , target.tollsAmount = source.tollsAmount\r\n",
        "WHEN NOT MATCHED\r\n",
        "  THEN INSERT (\r\n",
        "    target.vendorID, target.tpepPickupDateTime, target.tpepDropoffDateTime, target.passengerCount, target.tripDistance,\r\n",
        "    target.puLocationId, target.doLocationId, target.startLon, target.startLat, target.endLon, target.endLat, target.rateCodeId,\r\n",
        "    target.storeAndFwdFlag, target.paymentType, target.fareAmount, target.extra, target.mtaTax, target.improvementSurcharge, target.tipAmount,\r\n",
        "    target.tollsAmount, target.totalAmount, target.puYear, target.puMonth)   \r\n",
        " VALUES (\r\n",
        "   source.vendorID, source.tpepPickupDateTime, source.tpepDropoffDateTime, source.passengerCount, source.tripDistance,\r\n",
        "   source.puLocationId, source.doLocationId, source.startLon, source.startLat, source.endLon, source.endLat, source.rateCodeId, \r\n",
        "   source.storeAndFwdFlag, source.paymentType, source.fareAmount, source.extra, source.mtaTax, source.improvementSurcharge, source.tipAmount, \r\n",
        "   source.tollsAmount, source.totalAmount, source.puYear, source.puMonth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "SELECT puYear, puMonth, count(*)\r\n",
        "FROM deltademo.deltapartitionedtaxitrips \r\n",
        "GROUP BY puYear, puMonth\r\n",
        "ORDER BY puYear, puMonth;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from delta.tables import *\r\n",
        "\r\n",
        "deltaTable = DeltaTable.forPath(spark, \"/data/events/\")\r\n",
        "\r\n",
        "deltaTable.alias(\"events\").merge(\r\n",
        "    updatesDF.alias(\"updates\"),\r\n",
        "    \"events.eventId = updates.eventId\") \\\r\n",
        "  .whenMatchedUpdate(set = { \"data\" : \"updates.data\" } ) \\\r\n",
        "  .whenNotMatchedInsert(values =\r\n",
        "    {\r\n",
        "      \"date\": \"updates.date\",\r\n",
        "      \"eventId\": \"updates.eventId\",\r\n",
        "      \"data\": \"updates.data\"\r\n",
        "    }\r\n",
        "  ) \\\r\n",
        "  .execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Time Travel - History of changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "DELETE FROM deltademo.deltapartitionedtaxitrips\r\n",
        "WHERE puYear = 2010 AND puMonth = 1;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "SELECT puYear, puMonth, count(*)\r\n",
        "FROM deltademo.deltapartitionedtaxitrips \r\n",
        "GROUP BY puYear, puMonth\r\n",
        "ORDER BY puYear, puMonth;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "--DESCRIBE HISTORY deltademo.deltapartitionedtaxitrips; -- LIMIT 1;  -- get the last operation only\r\n",
        "DESCRIBE HISTORY delta.`/data/deltademo/deltapartitionedtaxitrips`; "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from delta.tables import *\r\n",
        "deltapartitionedtaxitrips = DeltaTable.forPath(spark, '/data/deltademo/deltapartitionedtaxitrips')\r\n",
        "display(deltapartitionedtaxitrips.history())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "deltapartitionedtaxitrips = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/data/deltademo/deltapartitionedtaxitrips\")  \r\n",
        "display(deltapartitionedtaxitrips.groupBy(\"puYear\",\"puMonth\").count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "deltapartitionedtaxitrips = spark.read.format(\"delta\").option(\"timestampAsOf\", '2021-11-19 20:25:55.87').load(\"/data/deltademo/deltapartitionedtaxitrips\")  \r\n",
        "display(deltapartitionedtaxitrips.groupBy(\"puYear\",\"puMonth\").count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## VACCUM - Maintaining history of past data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "\r\n",
        "-- vacuum files not required by versions older than the default retention period\r\n",
        "VACUUM deltademo.deltapartitionedtaxitrips;\r\n",
        "\r\n",
        "-- vacuum files in path-based table\r\n",
        "VACUUM '/data/deltademo/deltapartitionedtaxitrips'; \r\n",
        "VACUUM delta.`/data/deltademo/deltapartitionedtaxitrips`;\r\n",
        "\r\n",
        "-- vacuum files not required by versions more than 720 hours (30 days) old\r\n",
        "VACUUM delta.`/data/deltademo/deltapartitionedtaxitrips` RETAIN 720 HOURS;\r\n",
        "\r\n",
        " -- do dry run to get the list of files to be deleted\r\n",
        "VACUUM deltademo.deltapartitionedtaxitrips DRY RUN;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from delta.tables import *\r\n",
        "\r\n",
        "deltaTable = DeltaTable.forPath(spark, '/data/deltademo/deltapartitionedtaxitrips')  # path-based tables, or\r\n",
        "deltaTable = DeltaTable.forName(spark, 'deltademo.deltapartitionedtaxitrips')    # Hive metastore-based tables\r\n",
        "\r\n",
        "deltaTable.vacuum(720)     # vacuum files not required by versions more than 720 hours (30 days) old\r\n",
        "\r\n",
        "deltaTable.vacuum()        # vacuum files not required by versions older than the default retention period"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## File Compaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "path = \"/data/deltademo/deltapartitionedtaxitrips\"\r\n",
        "partition = \"puYear = '2010' and puMonth = '3'\"\r\n",
        "numFilesPerPartition = 16\r\n",
        "\r\n",
        "(spark.read\r\n",
        " .format(\"delta\")\r\n",
        " .load(path)\r\n",
        " .where(partition)\r\n",
        " .repartition(numFilesPerPartition)\r\n",
        " .write\r\n",
        " .option(\"dataChange\", \"false\")\r\n",
        " .format(\"delta\")\r\n",
        " .mode(\"overwrite\")\r\n",
        " .option(\"replaceWhere\", partition)\r\n",
        " .save(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Convert a Delta table to a Parquet table\r\n",
        "\r\n",
        "You can easily convert a Delta table back to a Parquet table using the following steps:\r\n",
        "\r\n",
        "If you have performed Delta Lake operations that can change the data files (for example, delete or merge), run vacuum with retention of 0 hours to delete all data files that do not belong to the latest version of the table.\r\n",
        "Delete the _delta_log directory in the table directory.\r\n",
        "\r\n",
        "**NOTE: **Delta Lake has a safety check to prevent you from running a dangerous vacuum command. If you are certain that there are no operations being performed on this table that take longer than the retention interval you plan to specify, you can turn off this safety check by setting the Apache Spark configuration property spark.databricks.delta.retentionDurationCheck.enabled to false. You must choose an interval that is longer than the longest running concurrent transaction and the longest period that any stream can lag behind the most recent update to the table.\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from delta.tables import *\r\n",
        "\r\n",
        "deltaTable = DeltaTable.forPath(spark, '/data/deltademo/deltapartitionedtaxitrips')  # path-based tables, or\r\n",
        "deltaTable = DeltaTable.forName(spark, 'deltademo.deltapartitionedtaxitrips')    # Hive metastore-based tables\r\n",
        "\r\n",
        "deltaTable.vacuum(0)        # vacuum files not required by versions older than the default retention period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.conf.get(\"spark.databricks.delta.retentionDurationCheck.enabled\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}